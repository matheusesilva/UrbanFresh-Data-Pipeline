{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ec14fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame, functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23c430ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "518e5dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"UrbanFresh_Data_Pipeline\") \\\n",
    "        .config(\"spark.sql.adaptative.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5971c309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sales_data(spark, file_path):\n",
    "    logger.info(f\"Extracting sales data from {file_path}\")\n",
    "\n",
    "    expected_schema = StructType([\n",
    "        StructField(\"order_id\", StringType(), True),\n",
    "        StructField(\"customer_id\", StringType(), True),\n",
    "        StructField(\"product_name\", StringType(), True),\n",
    "        StructField(\"price\", StringType(), True),\n",
    "        StructField(\"quantity\", StringType(), True),\n",
    "        StructField(\"order_date\", StringType(), True),\n",
    "        StructField(\"region\", StringType(), True)\n",
    "    ])\n",
    "    try:\n",
    "        sales_df = spark.read.schema(expected_schema) \\\n",
    "            .csv(file_path, header=True, mode=\"PERMISSIVE\")\n",
    "        \n",
    "        logger.info(\"Sales data extracted successfully.\")\n",
    "\n",
    "        return sales_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting sales data from {file_path}: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ef23623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_data(spark):\n",
    "    data_dir = Path(\"data/raw\")\n",
    "    \n",
    "    try:\n",
    "        files = [\n",
    "            file_path\n",
    "            for file_path in data_dir.glob(\"*.csv\")\n",
    "            if \"orders\" in file_path.name.lower()\n",
    "        ]\n",
    "        if not files:\n",
    "            logger.warning(f\"No CSV files found in {data_dir}\")\n",
    "            \n",
    "            raise FileNotFoundError(\n",
    "                f\"No sales CSV files found in {data_dir.resolve()}\"\n",
    "            )\n",
    "                \n",
    "        logger.info(f\"Found {len(files)} CSV files in {data_dir}\")\n",
    "    \n",
    "        # Use a generator expression to create DataFrames for each sales file and then union them together\n",
    "        dataframes = (\n",
    "            extract_sales_data(spark, str(file_path))\n",
    "            for file_path in files\n",
    "        )\n",
    "        return reduce(DataFrame.unionByName, dataframes)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during data extraction: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33fe1fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_customer_id(df):\n",
    "    df_clean = df.withColumn(\n",
    "        \"customer_id\",\n",
    "        #Select only rows the are not using the standard format for customer_id\n",
    "        F.when((~F.col(\"customer_id\").startswith(\"CUST_\")) \\\n",
    "               & (F.col(\"customer_id\").rlike(\"\\\\d+\")), \\\n",
    "                F.concat(\n",
    "                    F.lit(\"CUST_\"), \n",
    "                    F.regexp_extract(F.col(\"customer_id\"), \"\\\\d+\", 0))) \\\n",
    "         .otherwise(F.col(\"customer_id\"))\n",
    "    )\n",
    "    logger.info(f\"{df.count() - df_clean.count()} records had their customer_id cleaned.\")\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "846da341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_price_column(df):\n",
    "    df = df.withColumns({\n",
    "        \"unit_price\": \n",
    "        F.when(\n",
    "            F.col(\"price\").isNull(),\n",
    "            F.lit(0.0).cast(\"double\")\n",
    "        ).otherwise(\n",
    "            F.regexp_replace(F.col(\"price\"), \"[^0-9.]\", \"\")\n",
    "             .cast(\"double\")\n",
    "        )\n",
    "    })\n",
    "\n",
    "    df = df.withColumns({\n",
    "        \"price_quality_flag\":\n",
    "            F.when(F.col(\"unit_price\") < 0, \"CHECK_NEGATIVE_PRICE\")\n",
    "             .when(F.col(\"unit_price\") == 0, \"CHECK_ZERO_PRICE\")\n",
    "             .when(F.col(\"unit_price\") > 1000, \"CHECK_HIGH_PRICE\")\n",
    "             .otherwise(\"OK\")\n",
    "    })\n",
    "\n",
    "    logger.info(f\"Price column cleaned. {df.filter(F.col('price_quality_flag') != 'OK').count()} records flagged for review.\")\n",
    "\n",
    "    return df.drop(\"price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1885c003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_date_column(df):\n",
    "    dt1 = F.to_date(F.col(\"order_date\"), \"yyyy/MM/dd\")\n",
    "    dt2 = F.to_date(F.col(\"order_date\"), \"MM-dd-yyyy\")\n",
    "    dt3 = F.to_date(F.col(\"order_date\"), \"dd-MM-yyyy\")\n",
    "    dt4 = F.to_date(F.col(\"order_date\"), \"yyyy-MM-dd\")\n",
    "    dt5 = F.to_date(F.col(\"order_date\"), \"MM/dd/yyyy\")\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"order_date\",\n",
    "        F.coalesce(dt1, dt2, dt3, dt4, dt5)\n",
    "    )\n",
    "\n",
    "    logger.warning(f\"{df.filter(F.col('order_date').isNull()).count()} records with unparseable dates.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcb91fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_test_data(df):\n",
    "    df_cleaned = df.filter(\n",
    "            ~(\n",
    "                F.lower(F.col(\"customer_id\")).contains(\"test_\") |\n",
    "                F.lower(F.col(\"product_name\")).contains(\"test_\") |\n",
    "                F.col(\"order_id\").isNull() |\n",
    "                F.col(\"customer_id\").isNull()\n",
    "            )\n",
    "    )\n",
    "    logger.info(f\"{df.count()-df_cleaned.count()} records removed after filtering.\")\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4b0832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_duplicates(df):\n",
    "    df_deduped = df.dropDuplicates([\"order_id\"])\n",
    "    logger.info(f\"{df.count() - df_deduped.count()} duplicate records removed based on order_id.\")\n",
    "    return df_deduped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfb7b888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df):\n",
    "    return (\n",
    "        df\n",
    "        .transform(clean_customer_id)\n",
    "        .transform(clean_price_column)\n",
    "        .transform(standardize_date_column)\n",
    "        .transform(remove_test_data)\n",
    "        .withColumns({\n",
    "            \"quantity\": F.col(\"quantity\").cast(IntegerType()),\n",
    "            \"total_amount\": (F.col(\"unit_price\") * F.col(\"quantity\")).cast(DoubleType()),\n",
    "            \"processing_date\": F.current_date(),\n",
    "            \"year\": F.year(F.col(\"order_date\")),\n",
    "            \"month\": F.month(F.col(\"order_date\"))\n",
    "        })\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e860d83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0 records with unparseable dates.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+---------------------+--------+----------+------+----------+------------------+------------+---------------+----+-----+\n",
      "|order_id|customer_id|product_name         |quantity|order_date|region|unit_price|price_quality_flag|total_amount|processing_date|year|month|\n",
      "+--------+-----------+---------------------+--------+----------+------+----------+------------------+------------+---------------+----+-----+\n",
      "|MOB_3001|CUST_8821  |Whole Wheat Tortillas|2       |2024-10-15|North |3.99      |OK                |7.98        |2026-02-11     |2024|10   |\n",
      "|MOB_3002|CUST_1923  |Hummus               |1       |2024-10-16|South |5.5       |OK                |5.5         |2026-02-11     |2024|10   |\n",
      "|MOB_3003|CUST_4512  |Salsa                |2       |2024-10-17|East  |3.25      |OK                |6.5         |2026-02-11     |2024|10   |\n",
      "|MOB_3004|CUST_7634  |Guacamole            |1       |2024-10-18|West  |4.99      |OK                |4.99        |2026-02-11     |2024|10   |\n",
      "|MOB_3005|CUST_9123  |Tortilla Chips       |3       |2024-10-19|North |2.99      |OK                |8.97        |2026-02-11     |2024|10   |\n",
      "|MOB_3006|CUST_2345  |Black Beans          |4       |2024-10-20|South |1.5       |OK                |6.0         |2026-02-11     |2024|10   |\n",
      "|MOB_3007|CUST_5678  |Rice                 |1       |2024-10-21|East  |12.99     |OK                |12.99       |2026-02-11     |2024|10   |\n",
      "|MOB_3008|CUST_8901  |Sour Cream           |2       |2024-10-22|West  |3.75      |OK                |7.5         |2026-02-11     |2024|10   |\n",
      "|MOB_3009|CUST_3456  |Shredded Cheese      |1       |2024-10-23|North |4.5       |OK                |4.5         |2026-02-11     |2024|10   |\n",
      "|MOB_3010|CUST_6789  |Ground Beef          |1       |2024-10-24|South |9.99      |OK                |9.99        |2026-02-11     |2024|10   |\n",
      "|MOB_3012|CUST_1234  |Taco Shells          |2       |2024-10-25|East  |2.99      |OK                |5.98        |2026-02-11     |2024|10   |\n",
      "|MOB_3013|CUST_4567  |Hot Sauce            |1       |2024-10-26|West  |1.99      |OK                |1.99        |2026-02-11     |2024|10   |\n",
      "|MOB_3014|CUST_7890  |Cilantro             |3       |2024-10-27|North |1.25      |OK                |3.75        |2026-02-11     |2024|10   |\n",
      "|MOB_3015|CUST_9012  |Lime Juice           |2       |2024-10-28|South |2.5       |OK                |5.0         |2026-02-11     |2024|10   |\n",
      "|MOB_3016|CUST_2346  |Jalape√±os            |4       |2024-10-29|East  |1.99      |OK                |7.96        |2026-02-11     |2024|10   |\n",
      "|MOB_3017|CUST_5679  |Onion                |5       |2024-10-30|West  |0.99      |OK                |4.95        |2026-02-11     |2024|10   |\n",
      "|MOB_3018|CUST_8902  |Garlic               |2       |2024-10-31|North |1.5       |OK                |3.0         |2026-02-11     |2024|10   |\n",
      "|MOB_3019|CUST_3457  |Tomato Paste         |3       |2024-11-01|South |1.75      |OK                |5.25        |2026-02-11     |2024|11   |\n",
      "|MOB_3020|CUST_6781  |Chicken Thighs       |2       |2024-11-02|East  |7.99      |OK                |15.98       |2026-02-11     |2024|11   |\n",
      "|MOB_3021|CUST_1235  |Corn Tortillas       |2       |2024-11-03|West  |2.99      |OK                |5.98        |2026-02-11     |2024|11   |\n",
      "+--------+-----------+---------------------+--------+----------+------+----------+------------------+------------+---------------+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = extract_all_data(create_spark_session())\n",
    "df_transformed = transform_data(df)\n",
    "df_transformed.show(20, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
