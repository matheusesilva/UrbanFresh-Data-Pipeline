{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ec14fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame, functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from functools import reduce\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23c430ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "518e5dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"UrbanFresh_Data_Pipeline\") \\\n",
    "        .config(\"spark.sql.adaptative.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5971c309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sales_data(spark, file_path):\n",
    "    logger.info(f\"Extracting sales data from {file_path}\")\n",
    "\n",
    "    expected_schema = StructType([\n",
    "        StructField(\"order_id\", StringType(), True),\n",
    "        StructField(\"customer_id\", StringType(), True),\n",
    "        StructField(\"product_name\", StringType(), True),\n",
    "        StructField(\"price\", StringType(), True),\n",
    "        StructField(\"quantity\", StringType(), True),\n",
    "        StructField(\"order_date\", StringType(), True),\n",
    "        StructField(\"region\", StringType(), True)\n",
    "    ])\n",
    "    try:\n",
    "        sales_df = spark.read.schema(expected_schema) \\\n",
    "            .csv(file_path, header=True, mode=\"PERMISSIVE\")\n",
    "        \n",
    "        logger.info(\"Sales data extracted successfully.\")\n",
    "\n",
    "        return sales_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting sales data from {file_path}: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ef23623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_data(spark):\n",
    "    data_dir = Path(\"data/raw\")\n",
    "    \n",
    "    try:\n",
    "        files = [\n",
    "            file_path\n",
    "            for file_path in data_dir.glob(\"*.csv\")\n",
    "            if \"orders\" in file_path.name.lower()\n",
    "        ]\n",
    "        if not files:\n",
    "            logger.warning(f\"No CSV files found in {data_dir}\")\n",
    "            \n",
    "            raise FileNotFoundError(\n",
    "                f\"No sales CSV files found in {data_dir.resolve()}\"\n",
    "            )\n",
    "                \n",
    "        logger.info(f\"Found {len(files)} CSV files in {data_dir}\")\n",
    "    \n",
    "        # Use a generator expression to create DataFrames for each sales file and then union them together\n",
    "        dataframes = (\n",
    "            extract_sales_data(spark, str(file_path))\n",
    "            for file_path in files\n",
    "        )\n",
    "        return reduce(DataFrame.unionByName, dataframes)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during data extraction: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33fe1fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_customer_id(df):\n",
    "    df_clean = df.withColumn(\n",
    "        \"customer_id\",\n",
    "        #Select only rows the are not using the standard format for customer_id\n",
    "        F.when((~F.col(\"customer_id\").startswith(\"CUST_\")) \\\n",
    "               & (F.col(\"customer_id\").rlike(\"\\\\d+\")), \\\n",
    "                F.concat(\n",
    "                    F.lit(\"CUST_\"), \n",
    "                    F.regexp_extract(F.col(\"customer_id\"), \"\\\\d+\", 0))) \\\n",
    "         .otherwise(F.col(\"customer_id\"))\n",
    "    )\n",
    "    logger.info(f\"{df.count() - df_clean.count()} records had their customer_id cleaned.\")\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "846da341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_price_column(df):\n",
    "    df = df.withColumns({\n",
    "        \"unit_price\": \n",
    "        F.when(\n",
    "            F.col(\"price\").isNull(),\n",
    "            F.lit(0.0).cast(\"double\")\n",
    "        ).otherwise(\n",
    "            F.regexp_replace(F.col(\"price\"), \"[^0-9.]\", \"\")\n",
    "             .cast(\"double\")\n",
    "        )\n",
    "    })\n",
    "\n",
    "    df = df.withColumns({\n",
    "        \"price_quality_flag\":\n",
    "            F.when(F.col(\"unit_price\") < 0, \"CHECK_NEGATIVE_PRICE\")\n",
    "             .when(F.col(\"unit_price\") == 0, \"CHECK_ZERO_PRICE\")\n",
    "             .when(F.col(\"unit_price\") > 1000, \"CHECK_HIGH_PRICE\")\n",
    "             .otherwise(\"OK\")\n",
    "    })\n",
    "\n",
    "    logger.info(f\"Price column cleaned. {df.filter(F.col('price_quality_flag') != 'OK').count()} records flagged for review.\")\n",
    "\n",
    "    return df.drop(\"price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1885c003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_date_column(df):\n",
    "    dt1 = F.to_date(F.col(\"order_date\"), \"yyyy/MM/dd\")\n",
    "    dt2 = F.to_date(F.col(\"order_date\"), \"MM-dd-yyyy\")\n",
    "    dt3 = F.to_date(F.col(\"order_date\"), \"dd-MM-yyyy\")\n",
    "    dt4 = F.to_date(F.col(\"order_date\"), \"yyyy-MM-dd\")\n",
    "    dt5 = F.to_date(F.col(\"order_date\"), \"MM/dd/yyyy\")\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"order_date\",\n",
    "        F.coalesce(dt1, dt2, dt3, dt4, dt5)\n",
    "    )\n",
    "\n",
    "    logger.warning(f\"{df.filter(F.col('order_date').isNull()).count()} records with unparseable dates.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fcb91fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_test_data(df):\n",
    "    df_cleaned = df.filter(\n",
    "            ~(\n",
    "                F.lower(F.col(\"customer_id\")).contains(\"test\") |\n",
    "                F.lower(F.col(\"product_name\")).contains(\"test\") |\n",
    "                F.col(\"order_id\").isNull() |\n",
    "                F.col(\"customer_id\").isNull()\n",
    "            )\n",
    "    )\n",
    "    logger.info(f\"{df.count()-df_cleaned.count()} records removed after filtering.\")\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4b0832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_duplicates(df):\n",
    "    df_deduped = df.dropDuplicates([\"order_id\"])\n",
    "    logger.info(f\"{df.count() - df_deduped.count()} duplicate records removed based on order_id.\")\n",
    "    return df_deduped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfb7b888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df):\n",
    "    return (\n",
    "        df\n",
    "        .transform(clean_customer_id)\n",
    "        .transform(clean_price_column)\n",
    "        .transform(standardize_date_column)\n",
    "        .transform(remove_test_data)\n",
    "        .withColumns({\n",
    "            \"quantity\": F.col(\"quantity\").cast(IntegerType()),\n",
    "            \"total_amount\": F.format_number(\n",
    "                    (F.col(\"unit_price\") * F.col(\"quantity\")), 2\n",
    "                ).cast(DoubleType()),\n",
    "            \"processing_date\": F.current_date(),\n",
    "            \"year\": F.year(F.col(\"order_date\")),\n",
    "            \"month\": F.month(F.col(\"order_date\"))\n",
    "        })\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7edd7156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_csv(df, output_path):\n",
    "    if not os.path.exists(output_path):\n",
    "        logger.info(f\"Output directory {output_path} does not exist. Using default path /data/processed/orders.\")\n",
    "        output_path = \"data/processed/orders\"\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    pd_df = df.toPandas()\n",
    "\n",
    "    try:\n",
    "        pd_df.to_csv(f\"{output_path}/orders.csv\", index=False)\n",
    "        logger.info(f\"{len(pd_df)} records successfully loaded to {output_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data to {output_path}: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "599b9b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check_data(spark, output_path):\n",
    "    df = spark.read.csv(f\"{output_path}/orders.csv\", header=True, inferSchema=True)\n",
    "    df.createOrReplaceTempView(\"orders\")\n",
    "\n",
    "    total_records = spark.sql(\"SELECT COUNT(*) AS total_records FROM orders\").collect()[0][\"total_records\"]\n",
    "    logger.info(f\"Sanity check: {total_records} records in the output CSV.\")\n",
    "\n",
    "    zero_price_count = spark.sql(\"\"\"\n",
    "                                 SELECT COUNT(*) AS zero_price_count \n",
    "                                 FROM orders \n",
    "                                 WHERE unit_price = 0\n",
    "                                 \"\"\").collect()[0][\"zero_price_count\"]\n",
    "    if zero_price_count > 0:\n",
    "        logger.warning(f\"Sanity check: {zero_price_count} records with zero unit price in the output CSV.\")\n",
    "\n",
    "    data_range = spark.sql(\"\"\"\n",
    "                            SELECT MIN(order_date) AS min_date, MAX(order_date) AS max_date\n",
    "                            FROM orders\n",
    "                            \"\"\").collect()[0]\n",
    "    logger.info(f\"Sanity check: order date range from {data_range['min_date']} to {data_range['max_date']}.\")\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c7307543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_report(spark, output_path):\n",
    "    df = spark.read.csv(f\"{output_path}/orders.csv\", header=True, inferSchema=True)\n",
    "    df.createOrReplaceTempView(\"orders\")\n",
    "\n",
    "    summary_df = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            region,\n",
    "            year,\n",
    "            month,\n",
    "            COUNT(*) AS total_orders,\n",
    "            SUM(quantity) AS total_quantity,\n",
    "            ROUND(SUM(total_amount),2) AS total_revenue\n",
    "        FROM orders\n",
    "        GROUP BY region, year, month\n",
    "        ORDER BY year, month, region\n",
    "    \"\"\")\n",
    "    summary_output_path = f\"{output_path}/{datetime.now().strftime('%Y%m%d_%H%M%S')}-summary_report.csv\"\n",
    "    summary_pd_df = summary_df.toPandas()\n",
    "    try:\n",
    "        summary_pd_df.to_csv(summary_output_path, index=False)\n",
    "        logger.info(f\"Summary report successfully created at {summary_output_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating summary report at {summary_output_path}: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9b9277",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0 records with unparseable dates.\n"
     ]
    }
   ],
   "source": [
    "spark = create_spark_session()\n",
    "extracted_df = extract_all_data(spark)\n",
    "transformed_df = transform_data(extracted_df)\n",
    "load_to_csv(transformed_df, \"data/processed/orders\")\n",
    "sanity_check_data(spark, \"data/processed/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fdc74319",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_summary_report(spark, \"data/processed/orders\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
